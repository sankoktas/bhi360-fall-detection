{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Processed data (skipping first 0 samples) saved to 'mar_25_data_label_studio_input/DURURKEN-OTURMAK.json'.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "class NoNaNJSONEncoder(json.JSONEncoder):\n",
    "    \"\"\"A custom JSON encoder that replaces NaN/Inf with None (i.e., null).\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, float):\n",
    "            if np.isnan(obj) or np.isinf(obj):\n",
    "                return None  # becomes 'null' in the final JSON\n",
    "        return super().default(obj)\n",
    "\n",
    "def csv_to_labelstudio_json(input_csv: str, output_json: str, skip_count: int = 4):\n",
    "    \"\"\"\n",
    "    1) Reads the CSV (input_csv).\n",
    "    2) Merges timestamps from:\n",
    "       'sensortime_wakeup_ns' (unless '*') and\n",
    "       'sensortime_nonwakeup_ns'.\n",
    "    3) Renames sensor columns.\n",
    "    4) Filters columns to keep only:\n",
    "       {accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, timestamp_s}.\n",
    "    5) Skips the first skip_count samples from each column.\n",
    "    6) Builds the final JSON from remaining rows.\n",
    "    7) Removes rows (in the final JSON) if any sensor column is `'*'`.\n",
    "    8) Exports a single-task Label Studioâ€“compatible JSON (output_json).\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------\n",
    "    # STEP 1) READ CSV\n",
    "    # -----------------------\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # -----------------------\n",
    "    # STEP 2) MERGE TIMESTAMPS\n",
    "    # -----------------------\n",
    "    df[\"timestamp_ns\"] = df[\"sensortime_wakeup_ns\"].mask(\n",
    "        df[\"sensortime_wakeup_ns\"] == \"*\",\n",
    "        df[\"sensortime_nonwakeup_ns\"]\n",
    "    )\n",
    "    df[\"timestamp_ns\"] = pd.to_numeric(df[\"timestamp_ns\"], errors=\"coerce\")\n",
    "    df[\"timestamp_s\"] = df[\"timestamp_ns\"] / 1e9\n",
    "\n",
    "    # -----------------------\n",
    "    # STEP 3) RENAME COLUMNS\n",
    "    # -----------------------\n",
    "    rename_map = {\n",
    "        \"accelerometer_corrected_non_wakeup_ms2_x\": \"accel_x\",\n",
    "        \"accelerometer_corrected_non_wakeup_ms2_y\": \"accel_y\",\n",
    "        \"accelerometer_corrected_non_wakeup_ms2_z\": \"accel_z\",\n",
    "        \"gyroscope_corrected_non_wakeup_dps_x\": \"gyro_x\",\n",
    "        \"gyroscope_corrected_non_wakeup_dps_y\": \"gyro_y\",\n",
    "        \"gyroscope_corrected_non_wakeup_dps_z\": \"gyro_z\"\n",
    "    }\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # -----------------------\n",
    "    # STEP 4) FILTER COLUMNS\n",
    "    # -----------------------\n",
    "    columns_to_keep = [\n",
    "        \"accel_x\", \"accel_y\", \"accel_z\",\n",
    "        \"gyro_x\",  \"gyro_y\",  \"gyro_z\",\n",
    "        \"timestamp_s\"\n",
    "    ]\n",
    "    df_filtered = df[columns_to_keep]\n",
    "\n",
    "    # -----------------------\n",
    "    # STEP 5) SKIP FIRST N ROWS\n",
    "    # -----------------------\n",
    "    df_skipped = df_filtered.iloc[skip_count:].reset_index(drop=True)\n",
    "\n",
    "    # -----------------------\n",
    "    # STEP 6) PREPARE TO BUILD TIME SERIES DATA\n",
    "    # -----------------------\n",
    "    # We'll convert df_skipped to a list of dictionaries (one per row).\n",
    "    # Then we'll filter out rows where any sensor column equals '*'.\n",
    "    sensor_cols = [\"accel_x\", \"accel_y\", \"accel_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
    "    valid_rows = []\n",
    "\n",
    "    for _, row in df_skipped.iterrows():\n",
    "        if any(str(row[col]) == '*' for col in sensor_cols):\n",
    "            continue\n",
    "        valid_rows.append(row)\n",
    "\n",
    "    # -----------------------\n",
    "    # BUILD THE TIME-SERIES DATA FOR LABEL STUDIO\n",
    "    # -----------------------\n",
    "    # Construct each column's values as a list from the valid rows,\n",
    "    # converting sensor columns to float for numeric auto scaling.\n",
    "    timeseries_dict = {col: [] for col in df_skipped.columns}\n",
    "    for row in valid_rows:\n",
    "        for col in df_skipped.columns:\n",
    "            if col in sensor_cols:\n",
    "                timeseries_dict[col].append(float(row[col]))\n",
    "            else:\n",
    "                timeseries_dict[col].append(row[col])\n",
    "\n",
    "    tasks = [\n",
    "        {\n",
    "            \"data\": {\n",
    "                \"tsData\": timeseries_dict\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # -----------------------\n",
    "    # STEP 7) DUMP TO JSON\n",
    "    # -----------------------\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(tasks, f, indent=2, cls=NoNaNJSONEncoder)\n",
    "\n",
    "    display(f\"Processed data (skipping first {skip_count} samples) saved to '{output_json}'.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Example usage:\n",
    "# ------------------------------------------------------------------------------\n",
    "input_csv_path = \"mar_25_data/DURURKEN-OTURMAK.csv\"\n",
    "output_json_path = \"mar_25_data_label_studio_input/DURURKEN-OTURMAK.json\"\n",
    "skip_count = 0\n",
    "\n",
    "csv_to_labelstudio_json(\n",
    "    input_csv=input_csv_path,\n",
    "    output_json=output_json_path,\n",
    "    skip_count=skip_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# AT THIS STEP, IMPORT THE GIVEN JSON FILE TO LABEL STUDIO WITH THE FOLLOWING CONFIGURATION\n",
    "# ------------------------------------------------------------------------------\n",
    "#\n",
    "# <View>\n",
    "#   <TimeSeriesLabels name=\"tsLabels\" toName=\"ts\">\n",
    "#     <Label value=\"FALL\" background=\"red\"/>\n",
    "#     <Label value=\"MOTION\" background=\"#ffea00\"/>\n",
    "#     <Label value=\"NO MOTION\" background=\"#05ff16\"/>\n",
    "#   </TimeSeriesLabels>\n",
    "#\n",
    "#   <TimeSeries name=\"ts\" valueType=\"json\" value=\"$tsData\" timeColumn=\"timestamp_s\" ordered=\"true\">\n",
    "#     <Channel column=\"accel_x\" legend=\"accel_x\"/>\n",
    "#     <Channel column=\"accel_y\" legend=\"accel_y\"/>\n",
    "#     <Channel column=\"accel_z\" legend=\"accel_z\"/>\n",
    "#     <Channel column=\"gyro_x\" legend=\"gyro_x\"/>\n",
    "#     <Channel column=\"gyro_y\" legend=\"gyro_y\"/>\n",
    "#     <Channel column=\"gyro_z\" legend=\"gyro_z\"/>\n",
    "#   </TimeSeries>\n",
    "# </View>\n",
    "#\n",
    "# This configuration tells Label Studio to:\n",
    "#  - Use the \"timestamp_s\" column for the time axis,\n",
    "#  - Display multiple channels (accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z),\n",
    "#  - Provide labeling options like \"FALL\", \"MOTION\", and \"NO MOTION\".\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------\n",
    "# EXPORT THE LABELED DATA AS CSV FROM LABELSTUDIO AND IMPORT IT TO THE DATA DIRECTORY\n",
    "# ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Wrote row-based CSV with label to 'mar_25_data_label_studio_input/cleaned_training_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def convert_labelstudio_tsdata_with_timeserieslabels(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Reads a Label Studio exported CSV with columns:\n",
    "      - 'tsData' (JSON dict of arrays for sensor data)\n",
    "      - 'tsLabels' (JSON list of intervals, each like:\n",
    "           {\n",
    "             \"start\": float,\n",
    "             \"end\": float,\n",
    "             \"instant\": bool,\n",
    "             \"timeserieslabels\": [label]\n",
    "           }\n",
    "        )\n",
    "\n",
    "    Merges sensor data into row-based format (accel_x..timestamp_s) and assigns\n",
    "    a label among {STANDING, WALKING, TRANSITION, LAYING, SITTING, FALL}.\n",
    "    If no interval covers the timestamp, the row is removed.\n",
    "\n",
    "    Postprocessing:\n",
    "      - If a row has all sensor columns (accel_x..gyro_z) empty (None), it is removed.\n",
    "\n",
    "    Output columns:\n",
    "      accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, timestamp_s, label\n",
    "    \"\"\"\n",
    "\n",
    "    # Increase field size limit for large JSON in tsData/tsLabels columns\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    with open(input_csv, 'r', newline='', encoding='utf-8') as f_in:\n",
    "        reader = csv.DictReader(f_in)\n",
    "\n",
    "        for row in reader:\n",
    "            ts_str = row.get('tsData', '')\n",
    "            if not ts_str:\n",
    "                # No sensor data in this row, skip\n",
    "                continue\n",
    "\n",
    "            # Parse the sensor dictionary-of-arrays\n",
    "            try:\n",
    "                ts_dict = json.loads(ts_str)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Could not parse tsData JSON. Skipping. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Parse intervals from 'tsLabels'\n",
    "            intervals_str = row.get('tsLabels', '[]')\n",
    "            try:\n",
    "                intervals = json.loads(intervals_str)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Could not parse tsLabels JSON. Using empty intervals. Error: {e}\")\n",
    "                intervals = []\n",
    "\n",
    "            # Expected sensor keys in ts_dict\n",
    "            sensor_keys = [\n",
    "                \"accel_x\", \"accel_y\", \"accel_z\",\n",
    "                \"gyro_x\", \"gyro_y\", \"gyro_z\",\n",
    "                \"timestamp_s\"\n",
    "            ]\n",
    "            missing = [k for k in sensor_keys if k not in ts_dict]\n",
    "            if missing:\n",
    "                print(f\"Warning: Missing keys {missing} in tsData. Skipping row.\")\n",
    "                continue\n",
    "\n",
    "            # Convert dictionary-of-arrays -> row-based\n",
    "            length = len(ts_dict[\"timestamp_s\"])\n",
    "            for i in range(length):\n",
    "                row_data = {}\n",
    "                for k in sensor_keys:\n",
    "                    arr = ts_dict[k]\n",
    "                    row_data[k] = arr[i] if i < len(arr) else None\n",
    "\n",
    "                # Determine the label among our new set of labels; returns None if no interval covers timestamp.\n",
    "                label = determine_label_timeserieslabels(\n",
    "                    row_data[\"timestamp_s\"],\n",
    "                    intervals\n",
    "                )\n",
    "                # If no label is found, skip this row.\n",
    "                if label is None:\n",
    "                    continue\n",
    "\n",
    "                row_data[\"label\"] = label\n",
    "\n",
    "                # >>> POSTPROCESSING STEP <<<\n",
    "                # Skip if *all* sensor values are None/empty\n",
    "                sensor_values = [\n",
    "                    row_data[\"accel_x\"],\n",
    "                    row_data[\"accel_y\"],\n",
    "                    row_data[\"accel_z\"],\n",
    "                    row_data[\"gyro_x\"],\n",
    "                    row_data[\"gyro_y\"],\n",
    "                    row_data[\"gyro_z\"]\n",
    "                ]\n",
    "                if all(v is None for v in sensor_values):\n",
    "                    continue\n",
    "\n",
    "                # Otherwise, keep the row\n",
    "                all_rows.append(row_data)\n",
    "\n",
    "    # Write final CSV\n",
    "    out_cols = sensor_keys + [\"label\"]\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        writer = csv.DictWriter(f_out, fieldnames=out_cols)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_rows)\n",
    "\n",
    "\n",
    "def determine_label_timeserieslabels(timestamp, intervals):\n",
    "    \"\"\"\n",
    "    For each interval object in intervals, we have:\n",
    "      {\n",
    "        \"start\": float,\n",
    "        \"end\": float,\n",
    "        \"instant\": bool,\n",
    "        \"timeserieslabels\": [label]\n",
    "      }\n",
    "\n",
    "    The supported labels are:\n",
    "      STANDING, WALKING, TRANSITION, LAYING, SITTING, FALL\n",
    "\n",
    "    Priority within a matched [start, end) interval:\n",
    "      1) If 'FALL' in timeserieslabels -> label = 'FALL'\n",
    "      2) Else if 'TRANSITION' in timeserieslabels -> label = 'TRANSITION'\n",
    "      3) Else if 'LAYING' in timeserieslabels -> label = 'LAYING'\n",
    "      4) Else if 'SITTING' in timeserieslabels -> label = 'SITTING'\n",
    "      5) Else if 'WALKING' in timeserieslabels -> label = 'WALKING'\n",
    "      6) Else if 'STANDING' in timeserieslabels -> label = 'STANDING'\n",
    "    If no interval contains the timestamp, returns None.\n",
    "    \"\"\"\n",
    "    if timestamp is None:\n",
    "        return None\n",
    "\n",
    "    for interval in intervals:\n",
    "        start = interval.get(\"start\")\n",
    "        end = interval.get(\"end\")\n",
    "        ts_labels = interval.get(\"timeserieslabels\", [])\n",
    "        # Check if timestamp falls into [start, end)\n",
    "        if start is not None and end is not None and start <= timestamp < end:\n",
    "            labels_upper = [lbl.upper() for lbl in ts_labels]\n",
    "            if \"FALL\" in labels_upper:\n",
    "                return \"FALL\"\n",
    "            elif \"TRANSITION\" in labels_upper:\n",
    "                return \"TRANSITION\"\n",
    "            elif \"LAYING\" in labels_upper:\n",
    "                return \"LAYING\"\n",
    "            elif \"SITTING\" in labels_upper:\n",
    "                return \"SITTING\"\n",
    "            elif \"WALKING\" in labels_upper:\n",
    "                return \"WALKING\"\n",
    "            elif \"STANDING\" in labels_upper:\n",
    "                return \"STANDING\"\n",
    "            else:\n",
    "                # If interval exists but doesn't contain a recognized label,\n",
    "                # we treat it as no label found.\n",
    "                return None\n",
    "    # If no interval matched, return None so that the row can be removed.\n",
    "    return None\n",
    "\n",
    "\n",
    "# Example usage (when running as script, or adjust if using in a notebook):\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"mar_25_data_label_studio_input/merged_labeled_data.csv\"\n",
    "    output_file = \"mar_25_data_label_studio_input/cleaned_training_data.csv\"\n",
    "    convert_labelstudio_tsdata_with_timeserieslabels(input_file, output_file)\n",
    "    print(f\"Done! Wrote row-based CSV with label to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted data saved to 'mar_25_data_label_studio_input/cleaned_training_data_sorted.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sort_by_timestamp(input_csv, output_csv):\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Sort the DataFrame by 'timestamp_s' in ascending order\n",
    "    df_sorted = df.sort_values(by='timestamp_s', ascending=True)\n",
    "    \n",
    "    # Write the sorted data to a new CSV\n",
    "    df_sorted.to_csv(output_csv, index=False)\n",
    "    print(f\"Sorted data saved to '{output_csv}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"mar_25_data_label_studio_input/cleaned_training_data.csv\"\n",
    "    output_file = \"mar_25_data_label_studio_input/cleaned_training_data_sorted.csv\"\n",
    "    sort_by_timestamp(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 324217 rows from mar_25_data_label_studio_input/cleaned_training_data_sorted.csv\n",
      "\n",
      "Extracted 9253 window-level advanced feature rows.\n",
      "Columns in df_features: Index(['start_time', 'end_time', 'label', 'acc_xy_corr', 'acc_xz_corr',\n",
      "       'acc_yz_corr', 'gyro_xy_corr', 'gyro_xz_corr', 'gyro_yz_corr',\n",
      "       'acc_x_mean', 'acc_x_var', 'acc_x_skew', 'acc_x_kurt', 'acc_x_median',\n",
      "       'acc_x_range', 'acc_x_rms', 'acc_x_p10', 'acc_x_p25', 'acc_x_p50',\n",
      "       'acc_x_p75', 'acc_y_mean', 'acc_y_var', 'acc_y_skew', 'acc_y_kurt',\n",
      "       'acc_y_median', 'acc_y_range', 'acc_y_rms', 'acc_y_p10', 'acc_y_p25',\n",
      "       'acc_y_p50', 'acc_y_p75', 'acc_z_mean', 'acc_z_var', 'acc_z_skew',\n",
      "       'acc_z_kurt', 'acc_z_median', 'acc_z_range', 'acc_z_rms', 'acc_z_p10',\n",
      "       'acc_z_p25', 'acc_z_p50', 'acc_z_p75', 'gyro_x_mean', 'gyro_x_var',\n",
      "       'gyro_x_skew', 'gyro_x_kurt', 'gyro_x_median', 'gyro_x_range',\n",
      "       'gyro_x_rms', 'gyro_x_p10', 'gyro_x_p25', 'gyro_x_p50', 'gyro_x_p75',\n",
      "       'gyro_y_mean', 'gyro_y_var', 'gyro_y_skew', 'gyro_y_kurt',\n",
      "       'gyro_y_median', 'gyro_y_range', 'gyro_y_rms', 'gyro_y_p10',\n",
      "       'gyro_y_p25', 'gyro_y_p50', 'gyro_y_p75', 'gyro_z_mean', 'gyro_z_var',\n",
      "       'gyro_z_skew', 'gyro_z_kurt', 'gyro_z_median', 'gyro_z_range',\n",
      "       'gyro_z_rms', 'gyro_z_p10', 'gyro_z_p25', 'gyro_z_p50', 'gyro_z_p75',\n",
      "       'acc_x_fft_sum_amp', 'acc_x_fft_peak_idx', 'acc_y_fft_sum_amp',\n",
      "       'acc_y_fft_peak_idx', 'acc_z_fft_sum_amp', 'acc_z_fft_peak_idx',\n",
      "       'gyro_x_fft_sum_amp', 'gyro_x_fft_peak_idx', 'gyro_y_fft_sum_amp',\n",
      "       'gyro_y_fft_peak_idx', 'gyro_z_fft_sum_amp', 'gyro_z_fft_peak_idx'],\n",
      "      dtype='object')\n",
      "\n",
      "=== Quick RF Feature Importance Ranking ===\n",
      " 1)         acc_x_median: 0.0498\n",
      " 2)            acc_x_p50: 0.0449\n",
      " 3)           acc_x_mean: 0.0367\n",
      " 4)            acc_x_p25: 0.0358\n",
      " 5)            acc_x_rms: 0.0336\n",
      " 6)            acc_x_p10: 0.0335\n",
      " 7)            acc_x_p75: 0.0334\n",
      " 8)    acc_x_fft_sum_amp: 0.0330\n",
      " 9)          acc_x_range: 0.0322\n",
      "10)            acc_x_var: 0.0321\n",
      "(...only showing top 10)\n",
      "\n",
      "\n",
      "Train windows (time-based): 7402, Test windows (time-based): 1851\n",
      "\n",
      "Posture label encoder classes: ['LAYING' 'SITTING' 'STANDING' 'TRANSITION' 'WALKING']\n",
      "\n",
      "=== [Direct] Posture Classifier (Ignoring FALL) on Test ===\n",
      "Confusion Matrix (encoded):\n",
      "[[368   9  17 103   0]\n",
      " [161 222   6  60   0]\n",
      " [  0  28 401   5   0]\n",
      " [ 51  17  25 319  15]\n",
      " [  0   0   0   0   0]]\n",
      "Classification Report (encoded):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      LAYING       0.63      0.74      0.68       497\n",
      "     SITTING       0.80      0.49      0.61       449\n",
      "    STANDING       0.89      0.92      0.91       434\n",
      "  TRANSITION       0.66      0.75      0.70       427\n",
      "     WALKING       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.72      1807\n",
      "   macro avg       0.60      0.58      0.58      1807\n",
      "weighted avg       0.74      0.72      0.72      1807\n",
      "\n",
      "\n",
      "=== [Direct] Fall vs. Not-Fall Classifier on Test (Threshold=0.20) ===\n",
      "Confusion Matrix (binary): [ [TN, FP], [FN, TP] ]\n",
      "[[1437  370]\n",
      " [  12   32]]\n",
      "Classification Report (binary):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.80      0.88      1807\n",
      "           1       0.08      0.73      0.14        44\n",
      "\n",
      "    accuracy                           0.79      1851\n",
      "   macro avg       0.54      0.76      0.51      1851\n",
      "weighted avg       0.97      0.79      0.87      1851\n",
      "\n",
      "\n",
      "=== Markov Chain results (First 20 Windows) ===\n",
      "Win   0: real=SITTING, pred=SITTING\n",
      "Win   1: real=SITTING, pred=SITTING\n",
      "Win   2: real=SITTING, pred=SITTING\n",
      "Win   3: real=SITTING, pred=SITTING\n",
      "Win   4: real=SITTING, pred=SITTING\n",
      "Win   5: real=TRANSITION, pred=SITTING\n",
      "Win   6: real=TRANSITION, pred=TRANSITION\n",
      "Win   7: real=TRANSITION, pred=TRANSITION\n",
      "Win   8: real=TRANSITION, pred=TRANSITION\n",
      "Win   9: real=TRANSITION, pred=TRANSITION\n",
      "Win  10: real=TRANSITION, pred=TRANSITION\n",
      "Win  11: real=TRANSITION, pred=TRANSITION\n",
      "Win  12: real=TRANSITION, pred=TRANSITION\n",
      "Win  13: real=TRANSITION, pred=TRANSITION\n",
      "Win  14: real=TRANSITION, pred=TRANSITION\n",
      "Win  15: real=TRANSITION, pred=TRANSITION\n",
      "Win  16: real=TRANSITION, pred=TRANSITION\n",
      "Win  17: real=TRANSITION, pred=FALL\n",
      "Win  18: real=TRANSITION, pred=WALKING\n",
      "Win  19: real=TRANSITION, pred=WALKING\n",
      "\n",
      "=== Markov Chain Confusion Matrix (Window-Level) ===\n",
      "Order: ['STANDING', 'SITTING', 'LAYING', 'WALKING', 'TRANSITION', 'FALL']\n",
      "[[414  15   0   0   5   0]\n",
      " [  9 229 176   0  27   8]\n",
      " [  2   1 445   0  30  19]\n",
      " [  0   0   0   0   0   0]\n",
      " [ 37  24 110  30 198  28]\n",
      " [  3   0  20   2   4  15]]\n",
      "\n",
      "=== Markov Chain Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    STANDING       0.89      0.95      0.92       434\n",
      "     SITTING       0.85      0.51      0.64       449\n",
      "      LAYING       0.59      0.90      0.71       497\n",
      "     WALKING       0.00      0.00      0.00         0\n",
      "  TRANSITION       0.75      0.46      0.57       427\n",
      "        FALL       0.21      0.34      0.26        44\n",
      "\n",
      "    accuracy                           0.70      1851\n",
      "   macro avg       0.55      0.53      0.52      1851\n",
      "weighted avg       0.75      0.70      0.70      1851\n",
      "\n",
      "=== Overall Markov Chain Accuracy: 70.29% ===\n",
      "\n",
      "\n",
      "=== Pipeline Complete ===\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.fft import rfft\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Replace RandomOverSampler with SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "###############################################################################\n",
    "# 1) READ CSV\n",
    "###############################################################################\n",
    "\n",
    "def read_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.sort_values(by=\"timestamp_s\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "###############################################################################\n",
    "# 2) EXTRACT ADVANCED FEATURES\n",
    "###############################################################################\n",
    "\n",
    "def extract_advanced_features_from_timeseries(\n",
    "    df_timeseries,\n",
    "    window_size=0.150,\n",
    "    step_size=0.120,\n",
    "    do_fft=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits the time series into overlapping windows.\n",
    "    Each window => time-domain stats + optional FFT + correlations.\n",
    "    If ANY sample is 'FALL', the window label = 'FALL', else majority non-FALL.\n",
    "    \"\"\"\n",
    "    df = df_timeseries.copy()\n",
    "    df[\"timestamp_s\"] = df[\"timestamp_s\"].astype(float)\n",
    "\n",
    "    def safe_corr(a, b):\n",
    "        if len(a) > 1:\n",
    "            return np.corrcoef(a, b)[0,1]\n",
    "        return np.nan\n",
    "\n",
    "    def compute_time_stats(vals, prefix=\"acc_x_\"):\n",
    "        if len(vals) == 0:\n",
    "            return {\n",
    "                f\"{prefix}mean\": np.nan, f\"{prefix}var\": np.nan,\n",
    "                f\"{prefix}skew\": np.nan, f\"{prefix}kurt\": np.nan,\n",
    "                f\"{prefix}median\": np.nan, f\"{prefix}range\": np.nan,\n",
    "                f\"{prefix}rms\": np.nan, f\"{prefix}p10\": np.nan,\n",
    "                f\"{prefix}p25\": np.nan, f\"{prefix}p50\": np.nan, f\"{prefix}p75\": np.nan\n",
    "            }\n",
    "        mean_ = np.mean(vals)\n",
    "        var_  = np.var(vals)\n",
    "        skew_val = skew(vals)\n",
    "        kurt_val = kurtosis(vals)\n",
    "        med_  = np.median(vals)\n",
    "        rng   = np.max(vals) - np.min(vals)\n",
    "        rms_  = np.sqrt(np.mean(vals**2))\n",
    "        p10, p25, p50, p75 = np.percentile(vals, [10,25,50,75])\n",
    "        return {\n",
    "            f\"{prefix}mean\": mean_,\n",
    "            f\"{prefix}var\":  var_,\n",
    "            f\"{prefix}skew\": skew_val,\n",
    "            f\"{prefix}kurt\": kurt_val,\n",
    "            f\"{prefix}median\": med_,\n",
    "            f\"{prefix}range\": rng,\n",
    "            f\"{prefix}rms\":   rms_,\n",
    "            f\"{prefix}p10\":   p10,\n",
    "            f\"{prefix}p25\":   p25,\n",
    "            f\"{prefix}p50\":   p50,\n",
    "            f\"{prefix}p75\":   p75\n",
    "        }\n",
    "\n",
    "    def compute_fft_stats(vals, prefix=\"acc_x_\"):\n",
    "        if len(vals) < 2:\n",
    "            return {f\"{prefix}fft_sum_amp\": np.nan, f\"{prefix}fft_peak_idx\": np.nan}\n",
    "        fft_vals = rfft(vals)\n",
    "        mag = np.abs(fft_vals)\n",
    "        sum_amp = np.sum(mag)\n",
    "        peak_idx = np.argmax(mag)\n",
    "        return {\n",
    "            f\"{prefix}fft_sum_amp\": sum_amp,\n",
    "            f\"{prefix}fft_peak_idx\": peak_idx\n",
    "        }\n",
    "\n",
    "    start_time = df[\"timestamp_s\"].min()\n",
    "    end_time   = df[\"timestamp_s\"].max()\n",
    "\n",
    "    windows_data = []\n",
    "    current_start = start_time\n",
    "\n",
    "    while current_start <= end_time:\n",
    "        current_end = current_start + window_size\n",
    "        mask = (df[\"timestamp_s\"] >= current_start) & (df[\"timestamp_s\"] < current_end)\n",
    "        df_w = df[mask]\n",
    "\n",
    "        if len(df_w) > 0:\n",
    "            ax = df_w[\"accel_x\"].values\n",
    "            ay = df_w[\"accel_y\"].values\n",
    "            az = df_w[\"accel_z\"].values\n",
    "            gx = df_w[\"gyro_x\"].values\n",
    "            gy = df_w[\"gyro_y\"].values\n",
    "            gz = df_w[\"gyro_z\"].values\n",
    "\n",
    "            # time-domain stats\n",
    "            stats_ax = compute_time_stats(ax, \"acc_x_\")\n",
    "            stats_ay = compute_time_stats(ay, \"acc_y_\")\n",
    "            stats_az = compute_time_stats(az, \"acc_z_\")\n",
    "            stats_gx = compute_time_stats(gx, \"gyro_x_\")\n",
    "            stats_gy = compute_time_stats(gy, \"gyro_y_\")\n",
    "            stats_gz = compute_time_stats(gz, \"gyro_z_\")\n",
    "\n",
    "            # correlations\n",
    "            def sc(a, b): return safe_corr(a, b)\n",
    "            acc_xy_corr = sc(ax, ay)\n",
    "            acc_xz_corr = sc(ax, az)\n",
    "            acc_yz_corr = sc(ay, az)\n",
    "            gyro_xy_corr = sc(gx, gy)\n",
    "            gyro_xz_corr = sc(gx, gz)\n",
    "            gyro_yz_corr = sc(gy, gz)\n",
    "\n",
    "            # optional FFT\n",
    "            fft_ax = compute_fft_stats(ax, \"acc_x_\") if do_fft else {}\n",
    "            fft_ay = compute_fft_stats(ay, \"acc_y_\") if do_fft else {}\n",
    "            fft_az = compute_fft_stats(az, \"acc_z_\") if do_fft else {}\n",
    "            fft_gx = compute_fft_stats(gx, \"gyro_x_\") if do_fft else {}\n",
    "            fft_gy = compute_fft_stats(gy, \"gyro_y_\") if do_fft else {}\n",
    "            fft_gz = compute_fft_stats(gz, \"gyro_z_\") if do_fft else {}\n",
    "\n",
    "            # label\n",
    "            labs_in = df_w[\"label\"].unique()\n",
    "            if \"FALL\" in labs_in:\n",
    "                lab = \"FALL\"\n",
    "            else:\n",
    "                lab_counts = df_w[\"label\"].value_counts()\n",
    "                lab = lab_counts.idxmax()\n",
    "\n",
    "            row_dict = {\n",
    "                \"start_time\": current_start,\n",
    "                \"end_time\": current_end,\n",
    "                \"label\": lab,\n",
    "                \"acc_xy_corr\": acc_xy_corr,\n",
    "                \"acc_xz_corr\": acc_xz_corr,\n",
    "                \"acc_yz_corr\": acc_yz_corr,\n",
    "                \"gyro_xy_corr\": gyro_xy_corr,\n",
    "                \"gyro_xz_corr\": gyro_xz_corr,\n",
    "                \"gyro_yz_corr\": gyro_yz_corr\n",
    "            }\n",
    "            row_dict.update(stats_ax)\n",
    "            row_dict.update(stats_ay)\n",
    "            row_dict.update(stats_az)\n",
    "            row_dict.update(stats_gx)\n",
    "            row_dict.update(stats_gy)\n",
    "            row_dict.update(stats_gz)\n",
    "            row_dict.update(fft_ax)\n",
    "            row_dict.update(fft_ay)\n",
    "            row_dict.update(fft_az)\n",
    "            row_dict.update(fft_gx)\n",
    "            row_dict.update(fft_gy)\n",
    "            row_dict.update(fft_gz)\n",
    "\n",
    "            windows_data.append(row_dict)\n",
    "\n",
    "        current_start += step_size\n",
    "\n",
    "    df_feat = pd.DataFrame(windows_data)\n",
    "    df_feat.dropna(subset=[\"label\"], inplace=True)\n",
    "    return df_feat\n",
    "\n",
    "###############################################################################\n",
    "# 3) QUICK RF FEATURE IMPORTANCE => PRINT TOP 10\n",
    "###############################################################################\n",
    "\n",
    "def print_top_10_features(df_features):\n",
    "    df_non_fall = df_features[df_features[\"label\"] != \"FALL\"].copy()\n",
    "    all_feat_cols = [c for c in df_non_fall.columns if c not in [\"start_time\", \"end_time\", \"label\"]]\n",
    "    if len(all_feat_cols) == 0:\n",
    "        print(\"No feature columns found!\")\n",
    "        return\n",
    "\n",
    "    X = df_non_fall[all_feat_cols].values\n",
    "    y = df_non_fall[\"label\"].values\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    X_imp = imputer.fit_transform(X)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_imp, y_enc)\n",
    "\n",
    "    importances = rf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_10 = indices[:10]\n",
    "\n",
    "    print(\"\\n=== Quick RF Feature Importance Ranking ===\")\n",
    "    for rank, idx in enumerate(top_10, start=1):\n",
    "        print(f\"{rank:2d}) {all_feat_cols[idx]:>20s}: {importances[idx]:.4f}\")\n",
    "    print(\"(...only showing top 10)\\n\")\n",
    "\n",
    "###############################################################################\n",
    "# 4) DATA AUGMENTATION + CLASSIFIER\n",
    "###############################################################################\n",
    "\n",
    "def augment_with_noise(X, y, noise_level=0.01, num_copies=2):\n",
    "    X_list = [X]\n",
    "    y_list = [y]\n",
    "    std_dev = X.std(axis=0, keepdims=True)\n",
    "\n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0.0, scale=noise_level, size=X.shape)\n",
    "        X_noisy = X + noise * std_dev\n",
    "        X_list.append(X_noisy)\n",
    "        y_list.append(y)\n",
    "\n",
    "    X_aug = np.concatenate(X_list, axis=0)\n",
    "    y_aug = np.concatenate(y_list, axis=0)\n",
    "    return X_aug, y_aug\n",
    "\n",
    "# -------------------- SMOTE Oversampling Instead of RandomOverSampler -------------------- #\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_posture_classifier(df_features, feat_cols):\n",
    "    \"\"\"\n",
    "    Trains a 5-class posture classifier (LAYING, SITTING, STANDING,\n",
    "    TRANSITION, WALKING), ignoring FALL rows.\n",
    "    \"\"\"\n",
    "    df_non_fall = df_features[df_features[\"label\"] != \"FALL\"].copy()\n",
    "    X = df_non_fall[feat_cols].values\n",
    "    y = df_non_fall[\"label\"].values\n",
    "\n",
    "    imp = SimpleImputer(strategy=\"mean\")\n",
    "    X_no_nan = imp.fit_transform(X)\n",
    "\n",
    "    # Data augmentation with noise\n",
    "    X_aug, y_aug = augment_with_noise(X_no_nan, y, noise_level=0.01, num_copies=2)\n",
    "\n",
    "    # Use SMOTE instead of RandomOverSampler\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_aug, y_aug)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_res_enc = le.fit_transform(y_res)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_res, y_res_enc)\n",
    "\n",
    "    return clf, le\n",
    "\n",
    "def train_fall_classifier(df_features, feat_cols):\n",
    "    \"\"\"\n",
    "    Trains a binary fall vs. not-fall classifier.\n",
    "    \"\"\"\n",
    "    df_fall_bin = df_features.copy()\n",
    "    df_fall_bin[\"is_fall\"] = df_fall_bin[\"label\"].apply(lambda x: 1 if x == \"FALL\" else 0)\n",
    "\n",
    "    X = df_fall_bin[feat_cols].values\n",
    "    y = df_fall_bin[\"is_fall\"].values\n",
    "\n",
    "    imp = SimpleImputer(strategy=\"mean\")\n",
    "    X_no_nan = imp.fit_transform(X)\n",
    "\n",
    "    # Data augmentation with noise\n",
    "    X_aug, y_aug = augment_with_noise(X_no_nan, y, noise_level=0.01, num_copies=2)\n",
    "\n",
    "    # Use SMOTE instead of RandomOverSampler\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_aug, y_aug)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_res, y_res)\n",
    "    return clf\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) VITERBI CHAIN\n",
    "###############################################################################\n",
    "\n",
    "def build_transition_info_6():\n",
    "    \"\"\"\n",
    "    Returns a transition matrix and label list for 6 states:\n",
    "    STANDING, SITTING, LAYING, WALKING, TRANSITION, FALL.\n",
    "    \"\"\"\n",
    "    label_list_6 = [\"STANDING\",\"SITTING\",\"LAYING\",\"WALKING\",\"TRANSITION\",\"FALL\"]\n",
    "    trans_mat_6 = np.array([\n",
    "        [0.4, 0.0, 0.0, 0.2, 0.3, 0.1],\n",
    "        [0.0, 0.5, 0.0, 0.0, 0.4, 0.1],\n",
    "        [0.0, 0.0, 0.6, 0.0, 0.3, 0.1],\n",
    "        [0.3, 0.0, 0.0, 0.5, 0.0, 0.2],\n",
    "        [0.2, 0.2, 0.2, 0.0, 0.2, 0.2],\n",
    "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
    "    ], dtype=np.float64)\n",
    "    return trans_mat_6, label_list_6\n",
    "\n",
    "def combine_posteriors(posture_probs, fall_probs):\n",
    "    \"\"\"\n",
    "    Merges posture probabilities with fall probabilities into a single\n",
    "    6-state distribution per time-window, with no user-defined scaling factor.\n",
    "    \"\"\"\n",
    "    N = posture_probs.shape[0]\n",
    "    combined = np.zeros((N, 6))\n",
    "    for i in range(N):\n",
    "        p_fall = fall_probs[i]\n",
    "        # Clip just to be safe\n",
    "        if p_fall > 1.0:\n",
    "            p_fall = 1.0\n",
    "        elif p_fall < 0.0:\n",
    "            p_fall = 0.0\n",
    "\n",
    "        p_not_fall = 1.0 - p_fall\n",
    "\n",
    "        # First 5 columns => posture distribution scaled by (1 - p_fall)\n",
    "        combined[i, :5] = posture_probs[i, :] * p_not_fall\n",
    "\n",
    "        # 6th column => fall probability\n",
    "        combined[i, 5] = p_fall\n",
    "\n",
    "    return combined\n",
    "\n",
    "def viterbi_decode_6state(combined_probs, trans_mat_6):\n",
    "    N = combined_probs.shape[0]\n",
    "    n_states = 6\n",
    "    log_obs = np.log(combined_probs + 1e-12)\n",
    "    log_trans = np.log(trans_mat_6 + 1e-12)\n",
    "\n",
    "    viterbi_mat = np.full((N, n_states), -np.inf)\n",
    "    backpointer = np.zeros((N, n_states), dtype=int)\n",
    "\n",
    "    # Initialize\n",
    "    for s in range(n_states):\n",
    "        viterbi_mat[0,s] = log_obs[0,s]\n",
    "\n",
    "    # Viterbi recursion\n",
    "    for t in range(1, N):\n",
    "        for s in range(n_states):\n",
    "            max_val = -np.inf\n",
    "            best_prev = 0\n",
    "            for s_prev in range(n_states):\n",
    "                candidate = (\n",
    "                    viterbi_mat[t-1, s_prev]\n",
    "                    + log_trans[s_prev, s]\n",
    "                    + log_obs[t, s]\n",
    "                )\n",
    "                if candidate > max_val:\n",
    "                    max_val = candidate\n",
    "                    best_prev = s_prev\n",
    "            viterbi_mat[t,s] = max_val\n",
    "            backpointer[t,s] = best_prev\n",
    "\n",
    "    # Backtrack\n",
    "    best_path = []\n",
    "    last_state = np.argmax(viterbi_mat[N-1])\n",
    "    best_path.append(last_state)\n",
    "    for t in range(N-1, 0, -1):\n",
    "        last_state = backpointer[t, last_state]\n",
    "        best_path.append(last_state)\n",
    "    best_path.reverse()\n",
    "    return best_path\n",
    "\n",
    "def decode_path_to_labels(best_path, label_list_6):\n",
    "    return [label_list_6[s] for s in best_path]\n",
    "\n",
    "def evaluate_viterbi_accuracy(df_features):\n",
    "    label_order = [\"STANDING\",\"SITTING\",\"LAYING\",\"WALKING\",\"TRANSITION\",\"FALL\"]\n",
    "    y_true = df_features[\"label\"].values\n",
    "    y_pred = df_features[\"viterbi_6state\"].values\n",
    "\n",
    "    print(\"\\n=== Markov Chain Confusion Matrix (Window-Level) ===\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=label_order)\n",
    "    print(\"Order:\", label_order)\n",
    "    print(cm)\n",
    "\n",
    "    print(\"\\n=== Markov Chain Classification Report ===\")\n",
    "    print(classification_report(y_true, y_pred, labels=label_order, zero_division=0))\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"=== Overall Markov Chain Accuracy: {acc*100:.2f}% ===\\n\")\n",
    "\n",
    "###############################################################################\n",
    "# MAIN\n",
    "###############################################################################\n",
    "\n",
    "def main():\n",
    "    csv_path = \"mar_25_data_label_studio_input/cleaned_training_data_sorted.csv\"\n",
    "    df_raw = read_data(csv_path)\n",
    "    print(f\"Read {len(df_raw)} rows from {csv_path}\")\n",
    "\n",
    "    # Extract advanced features\n",
    "    df_features = extract_advanced_features_from_timeseries(\n",
    "        df_raw,\n",
    "        window_size=0.150,\n",
    "        step_size=0.120,\n",
    "        do_fft=True\n",
    "    )\n",
    "    print(f\"\\nExtracted {len(df_features)} window-level advanced feature rows.\")\n",
    "    print(\"Columns in df_features:\", df_features.columns)\n",
    "\n",
    "    # Just for reference => see top features\n",
    "    print_top_10_features(df_features)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TIME-BASED SPLIT (instead of random split)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort by time in case not already sorted.\n",
    "    df_features = df_features.sort_values(\"start_time\").reset_index(drop=True)\n",
    "    split_index = int(0.8 * len(df_features))  # 80% for training, last 20% for testing\n",
    "\n",
    "    train_df = df_features.iloc[:split_index].copy()\n",
    "    test_df  = df_features.iloc[split_index:].copy()\n",
    "\n",
    "    print(f\"\\nTrain windows (time-based): {len(train_df)}, Test windows (time-based): {len(test_df)}\")\n",
    "\n",
    "    # We'll pick all feature columns except these\n",
    "    feat_cols = [c for c in df_features.columns if c not in [\"start_time\",\"end_time\",\"label\"]]\n",
    "\n",
    "    # Train posture + fall classifiers\n",
    "    posture_clf, posture_le = train_posture_classifier(train_df, feat_cols)\n",
    "    fall_clf = train_fall_classifier(train_df, feat_cols)\n",
    "\n",
    "    # Show the actual label-encoder order for the posture classes:\n",
    "    print(\"\\nPosture label encoder classes:\", posture_le.classes_)\n",
    "\n",
    "    ################################################################\n",
    "    # 1) Evaluate posture classifier ignoring FALL\n",
    "    ################################################################\n",
    "    df_test_posture = test_df[test_df[\"label\"] != \"FALL\"].copy()\n",
    "    X_test_posture = df_test_posture[feat_cols].values\n",
    "    y_test_posture = df_test_posture[\"label\"].values\n",
    "\n",
    "    y_test_posture_enc = posture_le.transform(y_test_posture)\n",
    "    y_pred_posture_enc = posture_clf.predict(X_test_posture)\n",
    "\n",
    "    print(\"\\n=== [Direct] Posture Classifier (Ignoring FALL) on Test ===\")\n",
    "    print(\"Confusion Matrix (encoded):\")\n",
    "    print(confusion_matrix(y_test_posture_enc, y_pred_posture_enc))\n",
    "    print(\"Classification Report (encoded):\")\n",
    "    print(classification_report(\n",
    "        y_test_posture_enc,\n",
    "        y_pred_posture_enc,\n",
    "        zero_division=0,\n",
    "        target_names=posture_le.classes_\n",
    "    ))\n",
    "\n",
    "    ################################################################\n",
    "    # 2) Evaluate fall vs. not-fall classifier\n",
    "    #    USING A CUSTOM THRESHOLD of 0.20\n",
    "    ################################################################\n",
    "    X_test_fall = test_df[feat_cols].values\n",
    "    y_test_fall = np.array([1 if lbl == \"FALL\" else 0 for lbl in test_df[\"label\"].values])\n",
    "\n",
    "    # Get FALL probabilities from the model\n",
    "    fall_probs_test = fall_clf.predict_proba(X_test_fall)[:, 1]\n",
    "\n",
    "    # Use a threshold (0.20 shown here) to classify\n",
    "    y_pred_fall = (fall_probs_test >= 0.20).astype(int)\n",
    "\n",
    "    print(\"\\n=== [Direct] Fall vs. Not-Fall Classifier on Test (Threshold=0.20) ===\")\n",
    "    print(\"Confusion Matrix (binary): [ [TN, FP], [FN, TP] ]\")\n",
    "    print(confusion_matrix(y_test_fall, y_pred_fall))\n",
    "    print(\"Classification Report (binary):\")\n",
    "    print(classification_report(y_test_fall, y_pred_fall, zero_division=0))\n",
    "\n",
    "    ################################################################\n",
    "    # 3) Combine both into a 6-state Markov chain (posture + FALL)\n",
    "    ################################################################\n",
    "    posture_probs_test = posture_clf.predict_proba(X_test_fall)\n",
    "\n",
    "    # We want them in the order: [\"STANDING\",\"SITTING\",\"LAYING\",\"WALKING\",\"TRANSITION\"]\n",
    "    desired_5class_order = [\"STANDING\",\"SITTING\",\"LAYING\",\"WALKING\",\"TRANSITION\"]\n",
    "    class_to_idx = {label: i for i, label in enumerate(posture_le.classes_)}\n",
    "    reorder_indices = [class_to_idx[cls_name] for cls_name in desired_5class_order]\n",
    "    posture_probs_reordered = posture_probs_test[:, reorder_indices]\n",
    "\n",
    "    # We'll combine these posture probabilities with the raw fall probabilities\n",
    "    # (No scale factor)\n",
    "    fall_probs_2d = fall_clf.predict_proba(X_test_fall)\n",
    "    raw_fall_probs = fall_probs_2d[:, 1]\n",
    "\n",
    "    combined_6 = combine_posteriors(posture_probs_reordered, raw_fall_probs)\n",
    "\n",
    "    trans_mat_6, label_list_6 = build_transition_info_6()\n",
    "    best_path = viterbi_decode_6state(combined_6, trans_mat_6)\n",
    "    best_labels = decode_path_to_labels(best_path, label_list_6)\n",
    "\n",
    "    test_df[\"viterbi_6state\"] = best_labels\n",
    "\n",
    "    print(\"\\n=== Markov Chain results (First 20 Windows) ===\")\n",
    "    for i in range(min(20, len(test_df))):\n",
    "        real_lab = test_df[\"label\"].iloc[i]\n",
    "        pred_lab = test_df[\"viterbi_6state\"].iloc[i]\n",
    "        print(f\"Win {i:3d}: real={real_lab}, pred={pred_lab}\")\n",
    "\n",
    "    evaluate_viterbi_accuracy(test_df)\n",
    "    print(\"\\n=== Pipeline Complete ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
